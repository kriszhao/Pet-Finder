{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
      "env: CUDA_VISIBLE_DEVICES=0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import Dense, Reshape\n",
    "from keras.layers import Embedding, Dropout, LSTM, Bidirectional, Concatenate, Input\n",
    "from keras.models import Model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "%matplotlib inline\n",
    "%env CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
    "%env CUDA_VISIBLE_DEVICES=0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = 256\n",
    "batch_size = 16\n",
    "VALIDATION_SPLIT = 0.5\n",
    "TEST_SPLIT = 0.2\n",
    "\n",
    "#Sett=1\n",
    "MAX_SEQUENCE_LENGTH = 1000\n",
    "MAX_NB_WORDS = 20000\n",
    "EMBEDDING_DIM = 100\n",
    "\n",
    "#Measure of success\n",
    "def kappa(y_true, y_pred):\n",
    "    y_true = np.argmax(y_true, axis = 1)\n",
    "    y_pred = np.argmax(y_pred, axis = 1)\n",
    "    \n",
    "    return cohen_kappa_score(y_true, y_pred, weights='quadratic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(data):\n",
    "    # Apply binning to ages\n",
    "    data['Age'] = pd.cut(data['Age'], [-1, 2, 3, 6, 255], labels=[0, 1, 2, 3])\n",
    "\n",
    "    # Apply binning to fee\n",
    "    data['Fee'] = pd.cut(data['Fee'], [-1, 50, 100, 200, 3000], labels=[0, 1, 2, 3])\n",
    "\n",
    "    # Apply binning to photo amount\n",
    "    data['PhotoAmt'] = pd.cut(data['PhotoAmt'], [-1, 1, 5, 10, 100], labels=[0, 1, 2, 3])\n",
    "\n",
    "    # Apply binning to video amount\n",
    "    data['VideoAmt'] = pd.cut(data['VideoAmt'], [-1, 1, 100], labels=[0, 1])\n",
    "\n",
    "    # Replace names with 1 is present, 0 if not present\n",
    "    data.loc[data['Name'].notnull(), 'Name'] = 1\n",
    "    data.loc[data['Name'].isnull(), 'Name'] = 0\n",
    "\n",
    "    # Fill missing continuous data\n",
    "    data_continuous = data.select_dtypes(exclude=['object'])\n",
    "    data_continuous.fillna(0, inplace=True)\n",
    "\n",
    "    # Fill missing string data\n",
    "    data_categorical = data.select_dtypes(include=['object'])\n",
    "    data_categorical.fillna('NONE', inplace=True)\n",
    "\n",
    "    return data_continuous.merge(data_categorical, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/viteka/final_project/venv/lib/python3.6/site-packages/pandas/core/frame.py:3790: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  downcast=downcast, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "breeds = pd.read_csv('../all/breed_labels.csv')\n",
    "colors = pd.read_csv('../all/color_labels.csv')\n",
    "states = pd.read_csv('../all/state_labels.csv')\n",
    "train = prepare_data(pd.read_csv('../all/train.csv'))\n",
    "test = prepare_data(pd.read_csv('../all/test/test.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pet_ids = train['PetID'].values\n",
    "n_batches = len(pet_ids) // batch_size + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Extract text features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_str(df):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for dataset\n",
    "    Every dataset is lower cased except\n",
    "    \"\"\"\n",
    "    string = df['Description']\n",
    "    string = re.sub(r\"\\\\\", \"\", string)    \n",
    "    string = re.sub(r\"\\\"\", \"\", string)\n",
    "    string = re.sub(\"<a.*?</a>\", \"\", string) #remobe <a> href tag\n",
    "    return string.strip().lower()\n",
    "\n",
    "def preprocess(text):\n",
    "    \"\"\"\n",
    "    Preparing text and labels\n",
    "    \"\"\"\n",
    "    texts = []\n",
    "    \n",
    "    for idx in range(text.shape[0]):\n",
    "        text = df.Text[idx]\n",
    "        texts.append(clean_str(text))\n",
    "    \n",
    "    return texts\n",
    "\n",
    "def tokenization(texts,dataset= \"full_data\", maxlen=MAX_SEQUENCE_LENGTH,num_words=MAX_NB_WORDS):\n",
    "    ### tokenizing and creating word_index dictionary - Map each word to and index in vocabulary dictionary\n",
    "\n",
    "    tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "    tokenizer.fit_on_texts(texts)\n",
    "    sequences = tokenizer.texts_to_sequences(texts) #only keep top 20000\n",
    "\n",
    "    word_index = tokenizer.word_index # all unique word index!\n",
    "    print('Found {} unique tokens in {}.'.format(len(word_index),dataset))\n",
    "\n",
    "    data2 = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH) #pad all sequences to be size 1000\n",
    "    \n",
    "    return sequences, word_index, data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Description'] = train.apply(clean_str, axis = 1)\n",
    "test['Description'] = test.apply(clean_str, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 21810 unique tokens in full_data.\n",
      "Found 11547 unique tokens in full_data.\n"
     ]
    }
   ],
   "source": [
    "seq , word_index , full_data = tokenization(train.Description)\n",
    "test_seq , test_word_index , test_full_data = tokenization(test.Description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14993, 1000)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embedding(dic, word_embed):\n",
    "    embedin_matrix = np.random.random((len(dic) + 1 , EMBEDDING_DIM))\n",
    "    for word, ind in dic.items():\n",
    "        embedding_vector = word_embed.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedin_matrix[ind] = embedding_vector ### word not found in glove will be initialized randomly\n",
    "    \n",
    "    return embedin_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "GLOVE_DIR = \"../all/data/glove\"\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt'), encoding=\"utf8\")\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "em_mat = create_embedding(word_index, embeddings_index)\n",
    "#test_em_mat = create_embedding(test_word_index, embeddings_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the keras embedding layer which we will use for all our experiments\n",
    "embedding_layer = Embedding(len(em_mat),\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[em_mat],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get 256 image features for each pet\n",
    "\n",
    "Don't need to run the model now. Directly load the csv features. Later, we could use a different pre-trained model or even re-train few layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def resize_to_square(im):\n",
    "#     old_size = im.shape[:2] # old_size is in (height, width) format\n",
    "#     ratio = float(img_size)/max(old_size)\n",
    "#     new_size = tuple([int(x*ratio) for x in old_size])\n",
    "#     # new_size should be in (width, height) format\n",
    "#     im = cv2.resize(im, (new_size[1], new_size[0]))\n",
    "#     delta_w = img_size - new_size[1]\n",
    "#     delta_h = img_size - new_size[0]\n",
    "#     top, bottom = delta_h//2, delta_h-(delta_h//2)\n",
    "#     left, right = delta_w//2, delta_w-(delta_w//2)\n",
    "#     color = [0, 0, 0]\n",
    "#     new_im = cv2.copyMakeBorder(im, top, bottom, left, right, cv2.BORDER_CONSTANT,value=color)\n",
    "#     return new_im\n",
    "\n",
    "# def load_image(path, pet_id):\n",
    "#     image = cv2.imread(f'{path}{pet_id}-1.jpg')\n",
    "#     new_image = resize_to_square(image)\n",
    "#     new_image = preprocess_input(new_image)\n",
    "#     return new_image\n",
    "\n",
    "# inp = Input((256,256,3))\n",
    "# backbone = DenseNet121(input_tensor = inp, include_top = False)\n",
    "# x = backbone.output\n",
    "# x = GlobalAveragePooling2D()(x)\n",
    "# x = Lambda(lambda x: K.expand_dims(x,axis = -1))(x)\n",
    "# x = AveragePooling1D(4)(x)\n",
    "# out = Lambda(lambda x: x[:,:,0])(x)\n",
    "\n",
    "# m = Model(inp,out)\n",
    "\n",
    "# features = {}\n",
    "# for b in (range(n_batches)):\n",
    "#     start = b*batch_size\n",
    "#     end = (b+1)*batch_size\n",
    "#     batch_pets = pet_ids[start:end]\n",
    "#     batch_images = np.zeros((len(batch_pets),img_size,img_size,3))\n",
    "#     for i,pet_id in enumerate(batch_pets):\n",
    "#         try:\n",
    "#             batch_images[i] = load_image(\"all/train_images/\", pet_id)\n",
    "#         except:\n",
    "#             pass\n",
    "#     batch_preds = m.predict(batch_images)\n",
    "#     for i,pet_id in enumerate(batch_pets):\n",
    "#         features[pet_id] = batch_preds[i]\n",
    "\n",
    "# train_feats = pd.DataFrame.from_dict(features, orient='index')\n",
    "\n",
    "# train_feats.to_csv('test_feats = pd.DataFrame.from_dict(features, orient='index')\n",
    "# test_feats.to_csv('all/test/test_img_features.csv').csv')\n",
    "\n",
    "# test_df = pd.read_csv('all/test/test.csv')\n",
    "# pet_ids = test_df['PetID'].values\n",
    "# n_batches = len(pet_ids) // batch_size + 1\n",
    "\n",
    "# features = {}\n",
    "# for b in tqdm_notebook(range(n_batches)):\n",
    "#     start = b*batch_size\n",
    "#     end = (b+1)*batch_size\n",
    "#     batch_pets = pet_ids[start:end]\n",
    "#     batch_images = np.zeros((len(batch_pets),img_size,img_size,3))\n",
    "#     for i,pet_id in enumerate(batch_pets):\n",
    "#         try:\n",
    "#             batch_images[i] = load_image(\"all/test_images/\", pet_id)\n",
    "#         except:\n",
    "#             pass\n",
    "#     batch_preds = m.predict(batch_images)\n",
    "#     for i,pet_id in enumerate(batch_pets):\n",
    "#         features[pet_id] = batch_preds[i]\n",
    "\n",
    "# test_feats = pd.DataFrame.from_dict(features, orient='index')\n",
    "# test_feats.to_csv('all/test/test_img_features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feats = pd.read_csv('../all/train_img_features.csv')\n",
    "test_feats = pd.read_csv('../all/test/test_img_features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14993, 257)\n",
      "(3948, 257)\n"
     ]
    }
   ],
   "source": [
    "print(train_feats.shape)\n",
    "print(test_feats.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = train_feats.iloc[:, 1:]\n",
    "#train_feats = train_feats.drop(train_feats.columns[1:], axis = 1)\n",
    "#train_feats['Img_data'] = df.apply(lambda x: np.array(x), axis=1)\n",
    "#train_feats['Img_data'] = df.apply(lambda x: x.tolist(), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge the image features with the categorical and numerical embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Type</th>\n",
       "      <th>Name</th>\n",
       "      <th>Age</th>\n",
       "      <th>Breed1</th>\n",
       "      <th>Breed2</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Color1</th>\n",
       "      <th>Color2</th>\n",
       "      <th>Color3</th>\n",
       "      <th>MaturitySize</th>\n",
       "      <th>...</th>\n",
       "      <th>246</th>\n",
       "      <th>247</th>\n",
       "      <th>248</th>\n",
       "      <th>249</th>\n",
       "      <th>250</th>\n",
       "      <th>251</th>\n",
       "      <th>252</th>\n",
       "      <th>253</th>\n",
       "      <th>254</th>\n",
       "      <th>255</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>299</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.787699</td>\n",
       "      <td>0.176625</td>\n",
       "      <td>0.575706</td>\n",
       "      <td>1.088627</td>\n",
       "      <td>0.439557</td>\n",
       "      <td>0.520460</td>\n",
       "      <td>1.547071</td>\n",
       "      <td>0.832572</td>\n",
       "      <td>0.599095</td>\n",
       "      <td>0.763349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>265</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.628260</td>\n",
       "      <td>0.686865</td>\n",
       "      <td>0.563999</td>\n",
       "      <td>0.968190</td>\n",
       "      <td>1.070276</td>\n",
       "      <td>1.545739</td>\n",
       "      <td>0.894411</td>\n",
       "      <td>0.838595</td>\n",
       "      <td>0.468236</td>\n",
       "      <td>0.916672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>307</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.579116</td>\n",
       "      <td>0.557624</td>\n",
       "      <td>1.131405</td>\n",
       "      <td>0.720514</td>\n",
       "      <td>1.496672</td>\n",
       "      <td>0.870955</td>\n",
       "      <td>1.289682</td>\n",
       "      <td>1.184461</td>\n",
       "      <td>0.465113</td>\n",
       "      <td>0.892826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>307</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>1.295853</td>\n",
       "      <td>0.326143</td>\n",
       "      <td>0.291668</td>\n",
       "      <td>1.608086</td>\n",
       "      <td>1.119176</td>\n",
       "      <td>1.470888</td>\n",
       "      <td>0.591445</td>\n",
       "      <td>0.832753</td>\n",
       "      <td>0.483021</td>\n",
       "      <td>1.134128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>307</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>1.092663</td>\n",
       "      <td>0.669893</td>\n",
       "      <td>0.395784</td>\n",
       "      <td>0.886075</td>\n",
       "      <td>1.219730</td>\n",
       "      <td>1.033964</td>\n",
       "      <td>1.065685</td>\n",
       "      <td>0.304053</td>\n",
       "      <td>0.438069</td>\n",
       "      <td>0.676818</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 281 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Type  Name Age  Breed1  Breed2  Gender  Color1  Color2  Color3  \\\n",
       "0     2     1   1     299       0       1       1       7       0   \n",
       "1     2     1   0     265       0       1       1       2       0   \n",
       "2     1     1   0     307       0       1       2       7       0   \n",
       "3     1     1   2     307       0       2       1       2       0   \n",
       "4     1     1   0     307       0       1       1       0       0   \n",
       "\n",
       "   MaturitySize    ...          246       247       248       249       250  \\\n",
       "0             1    ...     0.787699  0.176625  0.575706  1.088627  0.439557   \n",
       "1             2    ...     0.628260  0.686865  0.563999  0.968190  1.070276   \n",
       "2             2    ...     0.579116  0.557624  1.131405  0.720514  1.496672   \n",
       "3             2    ...     1.295853  0.326143  0.291668  1.608086  1.119176   \n",
       "4             2    ...     1.092663  0.669893  0.395784  0.886075  1.219730   \n",
       "\n",
       "        251       252       253       254       255  \n",
       "0  0.520460  1.547071  0.832572  0.599095  0.763349  \n",
       "1  1.545739  0.894411  0.838595  0.468236  0.916672  \n",
       "2  0.870955  1.289682  1.184461  0.465113  0.892826  \n",
       "3  1.470888  0.591445  0.832753  0.483021  1.134128  \n",
       "4  1.033964  1.065685  0.304053  0.438069  0.676818  \n",
       "\n",
       "[5 rows x 281 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = train.merge(train_feats, left_on='PetID', right_on='Unnamed: 0', how='outer')\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.concat([train, pd.DataFrame(full_data)], axis=1, sort=False)\n",
    "test = pd.concat([test, pd.DataFrame(test_full_data)], axis=1, sort=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode tabular features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label = train.AdoptionSpeed\n",
    "#We drop name because it creates a huge embedding vector and we know that name is not very useful anyway\n",
    "train = train.drop(['AdoptionSpeed', 'Name', 'Description', 'PetID', 'Unnamed: 0', 'RescuerID'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14993, 1275)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create train and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(train, train_label, test_size=TEST_SPLIT, random_state=9)\n",
    "\n",
    "#Turn labels into n dimensional vectors for loss calculation\n",
    "y_train = to_categorical(y_train, num_classes=None)\n",
    "y_test = to_categorical(y_test, num_classes=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_vars = ['Type', 'Breed1', 'Breed2', 'Gender', 'Color1', 'Color2',\n",
    "        'Color3', 'MaturitySize', 'FurLength', 'Vaccinated', 'Dewormed',\n",
    "        'Sterilized', 'Health', 'State']\n",
    "numerical_vars = ['Age', 'Quantity', 'Fee', 'VideoAmt', 'PhotoAmt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preproc(X_train, X_test, embed_cols, num_cols):\n",
    "\n",
    "    input_list_train = []\n",
    "    input_list_test = []\n",
    "    m= MinMaxScaler()\n",
    "        \n",
    "    #the cols to be embedded: rescaling to range [0, # values)\n",
    "    for c in embed_cols:\n",
    "        raw_vals = np.unique(X_train[c])\n",
    "        val_map = {}\n",
    "        for i in range(len(raw_vals)):\n",
    "            val_map[raw_vals[i]] = i       \n",
    "        m.fit(X_train[c].map(val_map).values.reshape(-1, 1))\n",
    "        input_list_train.append(m.transform(X_train[c].map(val_map).values.reshape(-1, 1)))\n",
    "        \n",
    "        m.fit(X_test[c].map(val_map).fillna(0).values.reshape(-1, 1))\n",
    "        input_list_test.append(m.transform(X_test[c].map(val_map).fillna(0).values.reshape(-1, 1)))\n",
    "        \n",
    "    #the numerical columns\n",
    "    m.fit(X_train[num_cols].values)\n",
    "    input_list_train.append(m.transform(X_train[num_cols].values))\n",
    "    \n",
    "    m.fit(X_test[num_cols].values)\n",
    "    input_list_test.append(m.transform(X_test[num_cols].values))\n",
    "    \n",
    "    #img data\n",
    "    input_list_train.append(X_train.iloc[:, 19:275].as_matrix())\n",
    "    input_list_test.append(X_test.iloc[:, 19:275].as_matrix())\n",
    "    \n",
    "    #text data\n",
    "    input_list_train.append(X_train.iloc[:, 275:].as_matrix())\n",
    "    input_list_test.append(X_test.iloc[:, 275:].as_matrix())\n",
    "    \n",
    "    return input_list_train, input_list_test\n",
    "\n",
    "\n",
    "#Creating a Embedding model for categorical variables using the fast.ai approach\n",
    "def createModel(data, categorical_vars, numerical_vars):\n",
    "    embeddings = []\n",
    "    inputs = []\n",
    "    for categorical_var in categorical_vars :\n",
    "        i = Input(shape=(1,))\n",
    "        no_of_unique_cat  = data[categorical_var].nunique()\n",
    "        embedding_size = min(np.ceil((no_of_unique_cat)/2), 50 )\n",
    "        embedding_size = int(embedding_size)\n",
    "        vocab  = no_of_unique_cat+1\n",
    "        embedding = Embedding(vocab ,embedding_size, input_length = 1 )(i)\n",
    "        embedding = Reshape(target_shape=(embedding_size,))(embedding)\n",
    "        embeddings.append( embedding )\n",
    "        inputs.append(i)\n",
    "        \n",
    "    input_numeric = Input(shape=(len(numerical_vars),))\n",
    "    embedding_numeric = Dense(16)(input_numeric) \n",
    "    \n",
    "    \n",
    "    inputs.append(input_numeric)\n",
    "    embeddings.append(embedding_numeric)\n",
    "    \n",
    "    x = Concatenate()(embeddings)\n",
    "    x = Dense(40, activation='relu')(x)\n",
    "    x = Dropout(.25)(x)\n",
    "    \n",
    "    #hardcoded for now\n",
    "    image_input = Input(shape=(256,))\n",
    "    inputs.append(image_input)\n",
    "    \n",
    "    y = Dense(80, activation='relu')(image_input)    \n",
    "    y = Dense(40, activation='relu')(y)\n",
    "    y = Dropout(.25)(y)\n",
    "    \n",
    "    #Words\n",
    "    w = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "    inputs.append(w)\n",
    "    w = embedding_layer(w)\n",
    "    w = Bidirectional(LSTM(40, recurrent_dropout=0.3))(w)\n",
    "    \n",
    "    z = Concatenate()([x, y, w])\n",
    "    \n",
    "    z = Dense(20, activation='relu')(z)\n",
    "    z = Dense(30, activation='relu')(z)\n",
    "    \n",
    "    output = Dense(5, activation='softmax')(z)\n",
    "\n",
    "    model = Model(inputs, output)\n",
    "    model.compile(metrics=['categorical_accuracy'], loss='categorical_crossentropy', optimizer='adam')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "X_train, X_test = preproc(X_train, X_test, categorical_vars, numerical_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_5 (InputLayer)            (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_6 (InputLayer)            (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_7 (InputLayer)            (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_8 (InputLayer)            (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_9 (InputLayer)            (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_10 (InputLayer)           (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_11 (InputLayer)           (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_12 (InputLayer)           (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_13 (InputLayer)           (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_14 (InputLayer)           (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 1, 1)         3           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, 1, 50)        8850        input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_4 (Embedding)         (None, 1, 50)        6800        input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_5 (Embedding)         (None, 1, 2)         8           input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_6 (Embedding)         (None, 1, 4)         32          input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_7 (Embedding)         (None, 1, 4)         32          input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_8 (Embedding)         (None, 1, 3)         21          input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_9 (Embedding)         (None, 1, 2)         10          input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_10 (Embedding)        (None, 1, 2)         8           input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_11 (Embedding)        (None, 1, 2)         8           input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_12 (Embedding)        (None, 1, 2)         8           input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_13 (Embedding)        (None, 1, 2)         8           input_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_14 (Embedding)        (None, 1, 2)         8           input_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_15 (Embedding)        (None, 1, 7)         105         input_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "input_15 (InputLayer)           (None, 5)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 1)            0           embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "reshape_2 (Reshape)             (None, 50)           0           embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "reshape_3 (Reshape)             (None, 50)           0           embedding_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "reshape_4 (Reshape)             (None, 2)            0           embedding_5[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "reshape_5 (Reshape)             (None, 4)            0           embedding_6[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "reshape_6 (Reshape)             (None, 4)            0           embedding_7[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "reshape_7 (Reshape)             (None, 3)            0           embedding_8[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "reshape_8 (Reshape)             (None, 2)            0           embedding_9[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "reshape_9 (Reshape)             (None, 2)            0           embedding_10[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "reshape_10 (Reshape)            (None, 2)            0           embedding_11[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "reshape_11 (Reshape)            (None, 2)            0           embedding_12[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "reshape_12 (Reshape)            (None, 2)            0           embedding_13[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "reshape_13 (Reshape)            (None, 2)            0           embedding_14[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "reshape_14 (Reshape)            (None, 7)            0           embedding_15[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 16)           96          input_15[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "input_16 (InputLayer)           (None, 256)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 149)          0           reshape_1[0][0]                  \n",
      "                                                                 reshape_2[0][0]                  \n",
      "                                                                 reshape_3[0][0]                  \n",
      "                                                                 reshape_4[0][0]                  \n",
      "                                                                 reshape_5[0][0]                  \n",
      "                                                                 reshape_6[0][0]                  \n",
      "                                                                 reshape_7[0][0]                  \n",
      "                                                                 reshape_8[0][0]                  \n",
      "                                                                 reshape_9[0][0]                  \n",
      "                                                                 reshape_10[0][0]                 \n",
      "                                                                 reshape_11[0][0]                 \n",
      "                                                                 reshape_12[0][0]                 \n",
      "                                                                 reshape_13[0][0]                 \n",
      "                                                                 reshape_14[0][0]                 \n",
      "                                                                 dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 80)           20560       input_16[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "input_17 (InputLayer)           (None, 1000)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 40)           6000        concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 40)           3240        dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 1000, 100)    2181100     input_17[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 40)           0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 40)           0           dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 80)           45120       embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 160)          0           dropout_1[0][0]                  \n",
      "                                                                 dropout_2[0][0]                  \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 20)           3220        concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 30)           630         dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 5)            155         dense_6[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 2,276,022\n",
      "Trainable params: 94,922\n",
      "Non-trainable params: 2,181,100\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = createModel(train, categorical_vars, numerical_vars)\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, we will train our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath=\"../checkpoints/weights_image_text_categorical.hdf6\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_categorical_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "earlystopped = EarlyStopping(monitor='val_categorical_accuracy', min_delta=0.0001, patience=10, verbose=0, mode='max')\n",
    "#callbacks_list = [checkpoint, earlystopped]\n",
    "callbacks_list = [checkpoint, earlystopped]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5997 samples, validate on 5997 samples\n",
      "Epoch 1/40\n",
      "5997/5997 [==============================] - 120s 20ms/step - loss: 1.4600 - categorical_accuracy: 0.3033 - val_loss: 1.4369 - val_categorical_accuracy: 0.3305\n",
      "\n",
      "Epoch 00001: val_categorical_accuracy improved from -inf to 0.33050, saving model to ../checkpoints/weights_image_text_categorical.hdf6\n",
      "Epoch 2/40\n",
      "5997/5997 [==============================] - 103s 17ms/step - loss: 1.4068 - categorical_accuracy: 0.3513 - val_loss: 1.4019 - val_categorical_accuracy: 0.3562\n",
      "\n",
      "Epoch 00002: val_categorical_accuracy improved from 0.33050 to 0.35618, saving model to ../checkpoints/weights_image_text_categorical.hdf6\n",
      "Epoch 3/40\n",
      "4672/5997 [======================>.......] - ETA: 21s - loss: 1.3734 - categorical_accuracy: 0.3726"
     ]
    }
   ],
   "source": [
    "hist = model.fit(X_train, y_train, batch_size=64 ,epochs=40, validation_split=VALIDATION_SPLIT, \n",
    "                 shuffle=True, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(hist.history['loss'])\n",
    "plt.plot(hist.history['val_loss'])\n",
    "plt.legend(['loss', 'val_loss'])\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(hist.history['categorical_accuracy'])\n",
    "plt.plot(hist.history['val_categorical_accuracy'])\n",
    "plt.legend(['categorical_accuracy', 'val_categorical_accuracy'])\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate our test set and find out Cohen's  quadratic weighte kappa for both Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = model.predict(X_test)\n",
    "kappa(y_test, test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred = model.predict(X_train)\n",
    "kappa(y_train, train_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load out best model and check metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('checkpoints/weights_image_text_categorical.hdf6')\n",
    "\n",
    "test_pred = model.predict(X_test)\n",
    "kappa(y_test, test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred = model.predict(X_train)\n",
    "kappa(y_train, train_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
