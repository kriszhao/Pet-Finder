{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-c10b19e33f84>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModelCheckpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEarlyStopping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mActivation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from keras import Sequential\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.layers import Activation\n",
    "from keras.layers import Dense, np\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# CONSTANTS\n",
    "HIDDEN_UNITS = [64, 32, 32, 16]\n",
    "LABEL = 'AdoptionSpeed'\n",
    "TRAINING_TEST_SPLIT = 0.2\n",
    "RANDOM_NUMBER_SEED = 42\n",
    "N_CLASSES = 5\n",
    "EPOCHS = 100\n",
    "TRAIN_BATCH_SIZE = 10\n",
    "TRAIN_FILENAME = 'weights.best.hdf5'\n",
    "\n",
    "np.random.seed(RANDOM_NUMBER_SEED)\n",
    "\n",
    "\n",
    "def prepare_data(data):\n",
    "    pet_id = data.PetID\n",
    "\n",
    "    # Remove unused features\n",
    "    data.drop(['RescuerID', 'Description', 'PetID', 'State'], axis=1, inplace=True)\n",
    "\n",
    "    # Apply binning to ages\n",
    "    data['Age'] = pd.cut(data['Age'], [-1, 2, 3, 6, 255], labels=[0, 1, 2, 3])\n",
    "\n",
    "    # Apply binning to fee\n",
    "    data['Fee'] = pd.cut(data['Fee'], [-1, 50, 100, 200, 3000], labels=[0, 1, 2, 3])\n",
    "\n",
    "    # Apply binning to photo amount\n",
    "    data['PhotoAmt'] = pd.cut(data['PhotoAmt'], [-1, 1, 5, 10, 100], labels=[0, 1, 2, 3])\n",
    "\n",
    "    # Apply binning to video amount\n",
    "    data['VideoAmt'] = pd.cut(data['VideoAmt'], [-1, 1, 100], labels=[0, 1])\n",
    "\n",
    "    # Replace names with 1 is present, 0 if not present\n",
    "    data.loc[data['Name'].notnull(), 'Name'] = 1\n",
    "    data.loc[data['Name'].isnull(), 'Name'] = 0\n",
    "\n",
    "    # Fill missing continuous data\n",
    "    data_continuous = data.select_dtypes(exclude=['object'])\n",
    "    data_continuous.fillna(0, inplace=True)\n",
    "\n",
    "    # Fill missing string data\n",
    "    data_categorical = data.select_dtypes(include=['object'])\n",
    "    data_categorical.fillna('NONE', inplace=True)\n",
    "\n",
    "    final_data = data_continuous.merge(data_categorical, left_index=True, right_index=True)\n",
    "\n",
    "    return final_data, data_categorical, data_continuous, pet_id, data.shape[1]\n",
    "\n",
    "\n",
    "def create_mlp(input_dim, output_dim, arch=None):\n",
    "    # Default mlp architecture\n",
    "    arch = arch if arch else HIDDEN_UNITS\n",
    "\n",
    "    # Setup densely connected NN architecture (MLP)\n",
    "    model = Sequential()\n",
    "    model.add(Dense(arch[0], input_dim=input_dim, activation='relu'),)\n",
    "\n",
    "    for output in arch[1:]:\n",
    "        model.add(Dense(output, activation='relu'))\n",
    "        model.add(Activation('relu'))\n",
    "\n",
    "    model.add(Dense(output_dim, activation='sigmoid'))\n",
    "\n",
    "    rmsprop = RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0)\n",
    "\n",
    "    model.compile(optimizer=rmsprop,\n",
    "                  loss='mse',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Import and split\n",
    "    train, train_categorical, train_continuous, train_pet_id, training_dimension = prepare_data(\n",
    "        pd.read_csv('../all/train.csv'))\n",
    "    test, test_categorical, test_continuous, test_pet_id, _ = prepare_data(pd.read_csv('../all/test/test.csv'))\n",
    "\n",
    "    # Remove the outliers\n",
    "    clf = IsolationForest(max_samples=100, random_state=RANDOM_NUMBER_SEED)\n",
    "    clf.fit(train_continuous)\n",
    "    y_no_outliers = clf.predict(train_continuous)\n",
    "    y_no_outliers = pd.DataFrame(y_no_outliers, columns=['Top'])\n",
    "\n",
    "    train_continuous = train_continuous.iloc[y_no_outliers[y_no_outliers['Top'] == 1].index.values]\n",
    "    train_continuous.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    train_categorical = train_categorical.iloc[y_no_outliers[y_no_outliers['Top'] == 1].index.values]\n",
    "    train_categorical.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    train = train.iloc[y_no_outliers[y_no_outliers['Top'] == 1].index.values]\n",
    "    train.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Extract columns\n",
    "    columns = list(train_continuous.columns)\n",
    "\n",
    "    features_continuous = list(train_continuous.columns)\n",
    "    features_continuous.remove(LABEL)\n",
    "\n",
    "    features_categorical = list(train_categorical.columns)\n",
    "\n",
    "    # Extract matrices\n",
    "    matrix_train = np.matrix(train_continuous)\n",
    "    matrix_test = np.matrix(test_continuous)\n",
    "    matrix_test_no_label = np.matrix(train_continuous.drop(LABEL, axis=1))\n",
    "    matrix_y = np.array(train.AdoptionSpeed)\n",
    "\n",
    "    # Scale data\n",
    "    y_scaler = MinMaxScaler()\n",
    "    y_scaler.fit(matrix_y.reshape(matrix_y.shape[0], 1))\n",
    "\n",
    "    train_scaler = MinMaxScaler()\n",
    "    train_scaler.fit(matrix_train)\n",
    "\n",
    "    test_scaler = MinMaxScaler()\n",
    "    test_scaler.fit(matrix_test_no_label)\n",
    "\n",
    "    matrix_train_scaled = pd.DataFrame(train_scaler.transform(matrix_train), columns=columns)\n",
    "    test_matrix_scaled = pd.DataFrame(test_scaler.transform(matrix_test), columns=features_continuous)\n",
    "\n",
    "    train[columns] = pd.DataFrame(train_scaler.transform(matrix_train), columns=columns)\n",
    "    test[features_continuous] = test_matrix_scaled\n",
    "\n",
    "    # Extract continuous and categorical features\n",
    "    engineered_features = []\n",
    "\n",
    "    for continuous_feature in features_continuous:\n",
    "        engineered_features.append(tf.contrib.layers.real_valued_column(continuous_feature))\n",
    "\n",
    "    for categorical_feature in features_categorical:\n",
    "        sparse_column = tf.contrib.layers.sparse_column_with_hash_bucket(categorical_feature, hash_bucket_size=1000)\n",
    "\n",
    "        engineered_features.append(tf.contrib.layers.embedding_column(sparse_id_column=sparse_column,\n",
    "                                                                      dimension=16,\n",
    "                                                                      combiner='sum'))\n",
    "\n",
    "    # Split training set data between train and test\n",
    "    x_train, x_test, y_train, y_test = train_test_split(train[features_continuous + features_categorical],\n",
    "                                                        train[LABEL],\n",
    "                                                        test_size=0.8,\n",
    "                                                        random_state=RANDOM_NUMBER_SEED)\n",
    "    # Convert back to DataFrame\n",
    "    y_train = pd.DataFrame(y_train, columns=[LABEL])\n",
    "    x_train = pd.DataFrame(x_train, columns=features_continuous + features_categorical) \\\n",
    "        .merge(y_train, left_index=True, right_index=True)\n",
    "\n",
    "    y_test = pd.DataFrame(y_test, columns=[LABEL])\n",
    "    x_test = pd.DataFrame(x_test, columns=features_continuous + features_categorical) \\\n",
    "        .merge(y_test, left_index=True, right_index=True)\n",
    "\n",
    "    # Labels must be one-hot encoded for loss='categorical_crossentropy'\n",
    "    y_train_onehot = to_categorical(y_train, N_CLASSES)\n",
    "    y_test_onehot = to_categorical(y_test, N_CLASSES)\n",
    "\n",
    "    # Get neural network architecture and save to disk\n",
    "    model = create_mlp(input_dim=training_dimension, output_dim=N_CLASSES)\n",
    "\n",
    "    with open(TRAIN_FILENAME, 'w') as f:\n",
    "        f.write(model.to_yaml())\n",
    "\n",
    "    # Output logs to tensorflow TensorBoard\n",
    "    # tensorboard = TensorBoard()\n",
    "\n",
    "    # only save model weights for best performing model\n",
    "    checkpoint = ModelCheckpoint(TRAIN_FILENAME,\n",
    "                                 monitor='val_acc',\n",
    "                                 verbose=1,\n",
    "                                 save_best_only=True)\n",
    "\n",
    "    # Stop training early if validation accuracy doesn't improve for long enough\n",
    "    early_stopping = EarlyStopping(monitor='val_acc', patience=10)\n",
    "\n",
    "    # Shuffle data for good measure before fitting\n",
    "    x_train, y_train_onehot = shuffle(x_train, y_train_onehot)\n",
    "\n",
    "    x_train, x_val, y_train_onehot, y_val_onehot = train_test_split(x_train, y_train_onehot,\n",
    "                                                                    test_size=TRAINING_TEST_SPLIT,\n",
    "                                                                    random_state=RANDOM_NUMBER_SEED)\n",
    "\n",
    "    model.fit(x_train, y_train_onehot, validation_data=(x_test, y_test_onehot), epochs=EPOCHS,\n",
    "              batch_size=TRAIN_BATCH_SIZE,\n",
    "              shuffle=True,\n",
    "              callbacks=[checkpoint, early_stopping])\n",
    "\n",
    "    print('\\nTesting ------------')\n",
    "    # Evaluate the model with the metrics we defined earlier\n",
    "    loss, accuracy = model.evaluate(x_val, y_val_onehot)\n",
    "\n",
    "    print('test loss: ', loss)\n",
    "    print('test accuracy: ', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UPDATED MODEL for PET From Andy's Original works.ipynb\r\n",
      "Untitled.ipynb\r\n",
      "\u001b[31mbreed_labels.csv\u001b[m\u001b[m*\r\n",
      "\u001b[31mcolor_labels.csv\u001b[m\u001b[m*\r\n",
      "\u001b[31mstate_labels.csv\u001b[m\u001b[m*\r\n",
      "\u001b[34mtest\u001b[m\u001b[m/\r\n",
      "\u001b[34mtest_images\u001b[m\u001b[m/\r\n",
      "\u001b[34mtest_metadata\u001b[m\u001b[m/\r\n",
      "\u001b[34mtest_sentiment\u001b[m\u001b[m/\r\n",
      "train.csv\r\n",
      "\u001b[34mtrain_images\u001b[m\u001b[m/\r\n",
      "\u001b[34mtrain_metadata\u001b[m\u001b[m/\r\n",
      "\u001b[34mtrain_sentiment\u001b[m\u001b[m/\r\n",
      "weights.best.hdf5\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
