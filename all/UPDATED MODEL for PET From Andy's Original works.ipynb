{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zekunzhao/venv/lib/python3.6/site-packages/pandas/core/frame.py:4024: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  downcast=downcast, **kwargs)\n",
      "/Users/zekunzhao/venv/lib/python3.6/site-packages/sklearn/ensemble/iforest.py:213: FutureWarning: default contamination parameter 0.1 will change in version 0.22 to \"auto\". This will change the predict method behavior.\n",
      "  FutureWarning)\n",
      "/Users/zekunzhao/venv/lib/python3.6/site-packages/sklearn/ensemble/iforest.py:223: FutureWarning: behaviour=\"old\" is deprecated and will be removed in version 0.22. Please use behaviour=\"new\", which makes the decision_function change to match other anomaly detection algorithm API.\n",
      "  FutureWarning)\n",
      "/Users/zekunzhao/venv/lib/python3.6/site-packages/sklearn/ensemble/iforest.py:417: DeprecationWarning: threshold_ attribute is deprecated in 0.20 and will be removed in 0.22.\n",
      "  \" be removed in 0.22.\", DeprecationWarning)\n",
      "/Users/zekunzhao/venv/lib/python3.6/site-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/Users/zekunzhao/venv/lib/python3.6/site-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype object was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/Users/zekunzhao/venv/lib/python3.6/site-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype object was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1079 samples, validate on 10795 samples\n",
      "Epoch 1/10\n",
      "1079/1079 [==============================] - 1s 620us/step - loss: 0.1161 - acc: 0.6719 - val_loss: 0.0756 - val_acc: 0.7255\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.72552, saving model to weights.best.hdf5\n",
      "Epoch 2/10\n",
      "1079/1079 [==============================] - 0s 353us/step - loss: 0.0647 - acc: 0.7702 - val_loss: 0.0538 - val_acc: 0.8008\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.72552 to 0.80083, saving model to weights.best.hdf5\n",
      "Epoch 3/10\n",
      "1079/1079 [==============================] - 0s 363us/step - loss: 0.0436 - acc: 0.8656 - val_loss: 0.0409 - val_acc: 0.8802\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.80083 to 0.88022, saving model to weights.best.hdf5\n",
      "Epoch 4/10\n",
      "1079/1079 [==============================] - 0s 317us/step - loss: 0.0296 - acc: 0.9184 - val_loss: 0.0256 - val_acc: 0.9335\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.88022 to 0.93349, saving model to weights.best.hdf5\n",
      "Epoch 5/10\n",
      "1079/1079 [==============================] - 0s 317us/step - loss: 0.0204 - acc: 0.9527 - val_loss: 0.0174 - val_acc: 0.9666\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.93349 to 0.96656, saving model to weights.best.hdf5\n",
      "Epoch 6/10\n",
      "1079/1079 [==============================] - 0s 321us/step - loss: 0.0142 - acc: 0.9703 - val_loss: 0.0134 - val_acc: 0.9715\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.96656 to 0.97147, saving model to weights.best.hdf5\n",
      "Epoch 7/10\n",
      "1079/1079 [==============================] - 0s 378us/step - loss: 0.0096 - acc: 0.9796 - val_loss: 0.0116 - val_acc: 0.9722\n",
      "\n",
      "Epoch 00007: val_acc improved from 0.97147 to 0.97221, saving model to weights.best.hdf5\n",
      "Epoch 8/10\n",
      "1079/1079 [==============================] - 0s 331us/step - loss: 0.0076 - acc: 0.9833 - val_loss: 0.0072 - val_acc: 0.9819\n",
      "\n",
      "Epoch 00008: val_acc improved from 0.97221 to 0.98194, saving model to weights.best.hdf5\n",
      "Epoch 9/10\n",
      "1079/1079 [==============================] - 0s 386us/step - loss: 0.0059 - acc: 0.9861 - val_loss: 0.0071 - val_acc: 0.9802\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.98194\n",
      "Epoch 10/10\n",
      "1079/1079 [==============================] - 0s 330us/step - loss: 0.0045 - acc: 0.9907 - val_loss: 0.0053 - val_acc: 0.9824\n",
      "\n",
      "Epoch 00010: val_acc improved from 0.98194 to 0.98240, saving model to weights.best.hdf5\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from keras import Sequential\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.constraints import maxnorm\n",
    "from keras.layers import Dropout, Dense, np\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.utils import shuffle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import cohen_kappa_score as kappa_score\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from skopt import gp_minimize\n",
    "from skopt.space import Real, Integer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "kappa_scorer = make_scorer(kappa_score)\n",
    "from keras.constraints import maxnorm\n",
    "import os\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import glob\n",
    "import cv2\n",
    "import os\n",
    "from keras.models import Sequential\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.layers.core import Activation\n",
    "from keras.layers.core import Dropout\n",
    "from keras.layers.core import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Input\n",
    "from keras.models import Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers.core import Dense\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import concatenate\n",
    "import numpy as np\n",
    "import argparse\n",
    "import locale\n",
    "from keras.utils import np_utils\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "np.random.seed(1337)  # for reproducibility\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.optimizers import RMSprop\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from keras import Sequential\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.constraints import maxnorm\n",
    "from keras.layers import Dropout, Dense, np\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.utils import shuffle\n",
    "# CONSTANTS\n",
    "HIDDEN_UNITS = [100, 50, 25, 12]\n",
    "LABEL = 'AdoptionSpeed'\n",
    "TRAINING_TEST_SPLIT = 0.2\n",
    "RANDOM_NUMBER_SEED = 42\n",
    "N_CLASSES = 5\n",
    "EPOCHS = 100\n",
    "TRAIN_BATCH_SIZE = 10\n",
    "TRAIN_FILENAME = 'weights.best.hdf5'\n",
    "\n",
    "np.random.seed(RANDOM_NUMBER_SEED)\n",
    "\n",
    "\n",
    "def prepare_data(data):\n",
    "    pet_id = data.PetID\n",
    "\n",
    "    # Remove unused features\n",
    "    data.drop(['RescuerID', 'Description', 'PetID', 'State'], axis=1, inplace=True)\n",
    "\n",
    "    # Apply binning to ages\n",
    "    data['Age'] = pd.cut(data['Age'], [-1, 2, 3, 6, 255], labels=[0, 1, 2, 3])\n",
    "\n",
    "    # Apply binning to fee\n",
    "    data['Fee'] = pd.cut(data['Fee'], [-1, 50, 100, 200, 3000], labels=[0, 1, 2, 3])\n",
    "\n",
    "    # Apply binning to photo amount\n",
    "    data['PhotoAmt'] = pd.cut(data['PhotoAmt'], [-1, 1, 5, 10, 100], labels=[0, 1, 2, 3])\n",
    "\n",
    "    # Apply binning to video amount\n",
    "    data['VideoAmt'] = pd.cut(data['VideoAmt'], [-1, 1, 100], labels=[0, 1])\n",
    "\n",
    "    # Replace names with 1 is present, 0 if not present\n",
    "    data.loc[data['Name'].notnull(), 'Name'] = 1\n",
    "    data.loc[data['Name'].isnull(), 'Name'] = 0\n",
    "\n",
    "    # Fill missing continuous data\n",
    "    data_continuous = data.select_dtypes(exclude=['object'])\n",
    "    data_continuous.fillna(0, inplace=True)\n",
    "\n",
    "    # Fill missing string data\n",
    "    data_categorical = data.select_dtypes(include=['object'])\n",
    "    data_categorical.fillna('NONE', inplace=True)\n",
    "\n",
    "    final_data = data_continuous.merge(data_categorical, left_index=True, right_index=True)\n",
    "\n",
    "    return final_data, data_categorical, data_continuous, pet_id, data.shape[1]\n",
    "\n",
    "\n",
    "def create_mlp(input_dim, output_dim, dropout=0.3, arch=None):\n",
    "    # Default mlp architecture\n",
    "    \n",
    "    rmsprop = RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0)\n",
    "    model = Sequential([\n",
    "        Dense(64, input_dim=20),\n",
    "        Activation('relu'),\n",
    "        Dense(32),\n",
    "        Activation('relu'),\n",
    "        Dense(32),\n",
    "        Activation('relu'),\n",
    "        Dense(16),\n",
    "        Activation('relu'),\n",
    "        Dense(5),\n",
    "        Activation('sigmoid'),\n",
    "    ])\n",
    "    model.compile(optimizer=rmsprop,\n",
    "                  loss='mse',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Import and split\n",
    "    train, train_categorical, train_continuous, train_pet_id, training_dimension = prepare_data(\n",
    "        pd.read_csv('../all/train.csv'))\n",
    "    test, test_categorical, test_continuous, test_pet_id, _ = prepare_data(pd.read_csv('../all/test/test.csv'))\n",
    "\n",
    "    # Remove the outliers\n",
    "    clf = IsolationForest(max_samples=100, random_state=RANDOM_NUMBER_SEED)\n",
    "    clf.fit(train_continuous)\n",
    "    y_no_outliers = clf.predict(train_continuous)\n",
    "    y_no_outliers = pd.DataFrame(y_no_outliers, columns=['Top'])\n",
    "\n",
    "    train_continuous = train_continuous.iloc[y_no_outliers[y_no_outliers['Top'] == 1].index.values]\n",
    "    train_continuous.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    train_categorical = train_categorical.iloc[y_no_outliers[y_no_outliers['Top'] == 1].index.values]\n",
    "    train_categorical.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    train = train.iloc[y_no_outliers[y_no_outliers['Top'] == 1].index.values]\n",
    "    train.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Extract columns\n",
    "    columns = list(train_continuous.columns)\n",
    "\n",
    "    features_continuous = list(train_continuous.columns)\n",
    "    features_continuous.remove(LABEL)\n",
    "\n",
    "    features_categorical = list(train_categorical.columns)\n",
    "\n",
    "    # Extract matrices\n",
    "    matrix_train = np.matrix(train_continuous)\n",
    "    matrix_test = np.matrix(test_continuous)\n",
    "    matrix_test_no_label = np.matrix(train_continuous.drop(LABEL, axis=1))\n",
    "    matrix_y = np.array(train.AdoptionSpeed)\n",
    "\n",
    "    # Scale data\n",
    "    y_scaler = MinMaxScaler()\n",
    "    y_scaler.fit(matrix_y.reshape(matrix_y.shape[0], 1))\n",
    "\n",
    "    train_scaler = MinMaxScaler()\n",
    "    train_scaler.fit(matrix_train)\n",
    "\n",
    "    test_scaler = MinMaxScaler()\n",
    "    test_scaler.fit(matrix_test_no_label)\n",
    "\n",
    "    matrix_train_scaled = pd.DataFrame(train_scaler.transform(matrix_train), columns=columns)\n",
    "    test_matrix_scaled = pd.DataFrame(test_scaler.transform(matrix_test), columns=features_continuous)\n",
    "\n",
    "    train[columns] = pd.DataFrame(train_scaler.transform(matrix_train), columns=columns)\n",
    "    test[features_continuous] = test_matrix_scaled\n",
    "\n",
    "    # Extract continuous and categorical features\n",
    "    engineered_features = []\n",
    "\n",
    "    for continuous_feature in features_continuous:\n",
    "        engineered_features.append(tf.contrib.layers.real_valued_column(continuous_feature))\n",
    "\n",
    "    for categorical_feature in features_categorical:\n",
    "        sparse_column = tf.contrib.layers.sparse_column_with_hash_bucket(categorical_feature, hash_bucket_size=1000)\n",
    "\n",
    "        engineered_features.append(tf.contrib.layers.embedding_column(sparse_id_column=sparse_column,\n",
    "                                                                      dimension=16,\n",
    "                                                                      combiner='sum'))\n",
    "\n",
    "    # Split training set data between train and test\n",
    "    x_train, x_test, y_train, y_test = train_test_split(train[features_continuous + features_categorical],\n",
    "                                                        train[LABEL],\n",
    "                                                        test_size=0.8,\n",
    "                                                        random_state=RANDOM_NUMBER_SEED)\n",
    "    # Convert back to DataFrame\n",
    "    y_train = pd.DataFrame(y_train, columns=[LABEL])\n",
    "    x_train = pd.DataFrame(x_train, columns=features_continuous + features_categorical) \\\n",
    "        .merge(y_train, left_index=True, right_index=True)\n",
    "\n",
    "    y_test = pd.DataFrame(y_test, columns=[LABEL])\n",
    "    x_test = pd.DataFrame(x_test, columns=features_continuous + features_categorical) \\\n",
    "        .merge(y_test, left_index=True, right_index=True)\n",
    "\n",
    "    # Labels must be one-hot encoded for loss='categorical_crossentropy'\n",
    "    y_train_onehot = to_categorical(y_train, N_CLASSES)\n",
    "    y_test_onehot = to_categorical(y_test, N_CLASSES)\n",
    "\n",
    "    # Get neural network architecture and save to disk\n",
    "    model = create_mlp(input_dim=training_dimension, output_dim=N_CLASSES, arch=HIDDEN_UNITS)\n",
    "\n",
    "    with open(TRAIN_FILENAME, 'w') as f:\n",
    "        f.write(model.to_yaml())\n",
    "\n",
    "    # Output logs to tensorflow TensorBoard\n",
    "    # tensorboard = TensorBoard()\n",
    "\n",
    "    # only save model weights for best performing model\n",
    "    checkpoint = ModelCheckpoint(TRAIN_FILENAME,\n",
    "                                 monitor='val_acc',\n",
    "                                 verbose=1,\n",
    "                                 save_best_only=True)\n",
    "\n",
    "    # Stop training early if validation accuracy doesn't improve for long enough\n",
    "    early_stopping = EarlyStopping(monitor='val_acc', patience=500)\n",
    "\n",
    "    # Shuffle data for good measure before fitting\n",
    "    x_train, y_train_onehot = shuffle(x_train, y_train_onehot)\n",
    "    \n",
    "    x_train, x_val,y_train_onehot, y_val_onehot  = train_test_split(x_train,y_train_onehot, test_size = 0.6, random_state = 23)\n",
    "    \n",
    "    model.fit(x_train, y_train_onehot,validation_data = (x_test,y_test_onehot),epochs=10, batch_size=15,shuffle=True,\n",
    "              callbacks=[checkpoint, early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing ------------\n",
      "1619/1619 [==============================] - 0s 21us/step\n",
      "test loss:  0.005193772645873709\n",
      "test accuracy:  0.9833230389129092\n"
     ]
    }
   ],
   "source": [
    "print('\\nTesting ------------')\n",
    "# Evaluate the model with the metrics we defined earlier\n",
    "loss, accuracy = model.evaluate(x_val, y_val_onehot)\n",
    "\n",
    "print('test loss: ', loss)\n",
    "print('test accuracy: ', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
