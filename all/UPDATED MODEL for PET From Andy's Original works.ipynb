{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Users/zekunzhao/venv/lib/python3.6/site-packages/pandas/core/frame.py:4024: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  downcast=downcast, **kwargs)\n",
      "/Users/zekunzhao/venv/lib/python3.6/site-packages/sklearn/ensemble/iforest.py:213: FutureWarning: default contamination parameter 0.1 will change in version 0.22 to \"auto\". This will change the predict method behavior.\n",
      "  FutureWarning)\n",
      "/Users/zekunzhao/venv/lib/python3.6/site-packages/sklearn/ensemble/iforest.py:223: FutureWarning: behaviour=\"old\" is deprecated and will be removed in version 0.22. Please use behaviour=\"new\", which makes the decision_function change to match other anomaly detection algorithm API.\n",
      "  FutureWarning)\n",
      "/Users/zekunzhao/venv/lib/python3.6/site-packages/sklearn/ensemble/iforest.py:417: DeprecationWarning: threshold_ attribute is deprecated in 0.20 and will be removed in 0.22.\n",
      "  \" be removed in 0.22.\", DeprecationWarning)\n",
      "/Users/zekunzhao/venv/lib/python3.6/site-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/Users/zekunzhao/venv/lib/python3.6/site-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype object was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/Users/zekunzhao/venv/lib/python3.6/site-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype object was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2698 samples, validate on 10795 samples\n",
      "Epoch 1/50\n",
      "2698/2698 [==============================] - 1s 230us/step - loss: 0.0807 - acc: 0.7650 - val_loss: 0.0419 - val_acc: 0.8636\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.86364, saving model to weights.best.hdf5\n",
      "Epoch 2/50\n",
      "2698/2698 [==============================] - 0s 153us/step - loss: 0.0273 - acc: 0.9277 - val_loss: 0.0238 - val_acc: 0.9167\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.86364 to 0.91672, saving model to weights.best.hdf5\n",
      "Epoch 3/50\n",
      "2698/2698 [==============================] - 0s 154us/step - loss: 0.0125 - acc: 0.9715 - val_loss: 0.0072 - val_acc: 0.9842\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.91672 to 0.98416, saving model to weights.best.hdf5\n",
      "Epoch 4/50\n",
      "2698/2698 [==============================] - 0s 153us/step - loss: 0.0053 - acc: 0.9893 - val_loss: 0.0058 - val_acc: 0.9861\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.98416 to 0.98610, saving model to weights.best.hdf5\n",
      "Epoch 5/50\n",
      "2698/2698 [==============================] - 0s 153us/step - loss: 0.0027 - acc: 0.9952 - val_loss: 0.0020 - val_acc: 0.9959\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.98610 to 0.99592, saving model to weights.best.hdf5\n",
      "Epoch 6/50\n",
      "2698/2698 [==============================] - 0s 153us/step - loss: 0.0015 - acc: 0.9963 - val_loss: 0.0013 - val_acc: 0.9966\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.99592 to 0.99657, saving model to weights.best.hdf5\n",
      "Epoch 7/50\n",
      "2698/2698 [==============================] - 0s 152us/step - loss: 0.0011 - acc: 0.9974 - val_loss: 0.0012 - val_acc: 0.9976\n",
      "\n",
      "Epoch 00007: val_acc improved from 0.99657 to 0.99759, saving model to weights.best.hdf5\n",
      "Epoch 8/50\n",
      "2698/2698 [==============================] - 0s 176us/step - loss: 8.2596e-04 - acc: 0.9978 - val_loss: 0.0011 - val_acc: 0.9970\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.99759\n",
      "Epoch 9/50\n",
      "2698/2698 [==============================] - 0s 157us/step - loss: 4.3641e-04 - acc: 0.9993 - val_loss: 9.2590e-04 - val_acc: 0.9976\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.99759\n",
      "Epoch 10/50\n",
      "2698/2698 [==============================] - 0s 165us/step - loss: 5.3493e-04 - acc: 0.9981 - val_loss: 0.0012 - val_acc: 0.9973\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.99759\n",
      "Epoch 11/50\n",
      "2698/2698 [==============================] - 0s 157us/step - loss: 9.5425e-04 - acc: 0.9967 - val_loss: 8.7532e-04 - val_acc: 0.9976\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.99759\n",
      "Epoch 12/50\n",
      "2698/2698 [==============================] - 0s 172us/step - loss: 1.9737e-04 - acc: 0.9996 - val_loss: 8.1791e-04 - val_acc: 0.9981\n",
      "\n",
      "Epoch 00012: val_acc improved from 0.99759 to 0.99805, saving model to weights.best.hdf5\n",
      "Epoch 13/50\n",
      "2698/2698 [==============================] - 0s 176us/step - loss: 1.7370e-04 - acc: 0.9996 - val_loss: 7.8542e-04 - val_acc: 0.9979\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.99805\n",
      "Epoch 14/50\n",
      "2698/2698 [==============================] - 0s 179us/step - loss: 3.1262e-04 - acc: 0.9989 - val_loss: 9.0299e-04 - val_acc: 0.9977\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.99805\n",
      "Epoch 15/50\n",
      "2698/2698 [==============================] - 0s 164us/step - loss: 2.9145e-04 - acc: 0.9993 - val_loss: 7.6613e-04 - val_acc: 0.9981\n",
      "\n",
      "Epoch 00015: val_acc improved from 0.99805 to 0.99815, saving model to weights.best.hdf5\n",
      "Epoch 16/50\n",
      "2698/2698 [==============================] - 0s 185us/step - loss: 5.0742e-05 - acc: 1.0000 - val_loss: 7.5594e-04 - val_acc: 0.9981\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.99815\n",
      "Epoch 17/50\n",
      "2698/2698 [==============================] - 0s 177us/step - loss: 2.7034e-04 - acc: 0.9993 - val_loss: 7.8961e-04 - val_acc: 0.9981\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.99815\n",
      "Epoch 18/50\n",
      "2698/2698 [==============================] - 0s 175us/step - loss: 8.2815e-05 - acc: 0.9996 - val_loss: 7.4241e-04 - val_acc: 0.9981\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.99815\n",
      "Epoch 19/50\n",
      "2698/2698 [==============================] - 0s 167us/step - loss: 7.3177e-04 - acc: 0.9974 - val_loss: 8.8845e-04 - val_acc: 0.9979\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.99815\n",
      "Epoch 20/50\n",
      "2698/2698 [==============================] - 0s 156us/step - loss: 2.5603e-04 - acc: 0.9993 - val_loss: 7.5677e-04 - val_acc: 0.9980\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.99815\n",
      "Epoch 21/50\n",
      "2698/2698 [==============================] - 0s 158us/step - loss: 1.2092e-04 - acc: 0.9996 - val_loss: 7.1836e-04 - val_acc: 0.9982\n",
      "\n",
      "Epoch 00021: val_acc improved from 0.99815 to 0.99824, saving model to weights.best.hdf5\n",
      "Epoch 22/50\n",
      "2698/2698 [==============================] - 0s 155us/step - loss: 6.7222e-04 - acc: 0.9978 - val_loss: 0.0010 - val_acc: 0.9971\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.99824\n",
      "Epoch 23/50\n",
      "2698/2698 [==============================] - 0s 160us/step - loss: 1.1982e-05 - acc: 1.0000 - val_loss: 6.8951e-04 - val_acc: 0.9983\n",
      "\n",
      "Epoch 00023: val_acc improved from 0.99824 to 0.99833, saving model to weights.best.hdf5\n",
      "Epoch 24/50\n",
      "2698/2698 [==============================] - 0s 154us/step - loss: 3.4450e-04 - acc: 0.9989 - val_loss: 8.9008e-04 - val_acc: 0.9973\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.99833\n",
      "Epoch 25/50\n",
      "2698/2698 [==============================] - 0s 156us/step - loss: 1.9446e-04 - acc: 0.9993 - val_loss: 6.8658e-04 - val_acc: 0.9983\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.99833\n",
      "Epoch 26/50\n",
      "2698/2698 [==============================] - 0s 161us/step - loss: 3.8930e-04 - acc: 0.9985 - val_loss: 7.3089e-04 - val_acc: 0.9982\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.99833\n",
      "Epoch 27/50\n",
      "2698/2698 [==============================] - 0s 184us/step - loss: 1.3236e-04 - acc: 0.9996 - val_loss: 6.8370e-04 - val_acc: 0.9983\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.99833\n",
      "Epoch 28/50\n",
      "2698/2698 [==============================] - 1s 186us/step - loss: 5.8719e-05 - acc: 0.9996 - val_loss: 6.9738e-04 - val_acc: 0.9982\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.99833\n",
      "Epoch 29/50\n",
      "2698/2698 [==============================] - 0s 172us/step - loss: 9.7252e-05 - acc: 0.9996 - val_loss: 7.0337e-04 - val_acc: 0.9982\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.99833\n",
      "Epoch 30/50\n",
      "2698/2698 [==============================] - 0s 169us/step - loss: 7.3913e-05 - acc: 1.0000 - val_loss: 7.1404e-04 - val_acc: 0.9981\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.99833\n",
      "Epoch 31/50\n",
      "2698/2698 [==============================] - 1s 188us/step - loss: 1.2627e-04 - acc: 0.9996 - val_loss: 7.4266e-04 - val_acc: 0.9981\n",
      "\n",
      "Epoch 00031: val_acc did not improve from 0.99833\n",
      "Epoch 32/50\n",
      "2698/2698 [==============================] - 0s 177us/step - loss: 2.6455e-05 - acc: 1.0000 - val_loss: 7.5049e-04 - val_acc: 0.9980\n",
      "\n",
      "Epoch 00032: val_acc did not improve from 0.99833\n",
      "Epoch 33/50\n",
      "2698/2698 [==============================] - 1s 187us/step - loss: 2.5143e-04 - acc: 0.9993 - val_loss: 6.9128e-04 - val_acc: 0.9982\n",
      "\n",
      "Epoch 00033: val_acc did not improve from 0.99833\n",
      "Epoch 34/50\n",
      "2698/2698 [==============================] - 1s 186us/step - loss: 2.0981e-04 - acc: 0.9993 - val_loss: 6.9723e-04 - val_acc: 0.9982\n",
      "\n",
      "Epoch 00034: val_acc did not improve from 0.99833\n",
      "Epoch 35/50\n",
      "2698/2698 [==============================] - 0s 158us/step - loss: 2.1343e-07 - acc: 1.0000 - val_loss: 6.7646e-04 - val_acc: 0.9983\n",
      "\n",
      "Epoch 00035: val_acc did not improve from 0.99833\n",
      "Epoch 36/50\n",
      "2698/2698 [==============================] - 1s 189us/step - loss: 5.5039e-05 - acc: 0.9996 - val_loss: 7.0368e-04 - val_acc: 0.9982\n",
      "\n",
      "Epoch 00036: val_acc did not improve from 0.99833\n",
      "Epoch 37/50\n",
      "2698/2698 [==============================] - 1s 194us/step - loss: 2.2683e-05 - acc: 1.0000 - val_loss: 7.2405e-04 - val_acc: 0.9981\n",
      "\n",
      "Epoch 00037: val_acc did not improve from 0.99833\n",
      "Epoch 38/50\n",
      "2698/2698 [==============================] - 0s 179us/step - loss: 4.1748e-06 - acc: 1.0000 - val_loss: 8.6404e-04 - val_acc: 0.9977\n",
      "\n",
      "Epoch 00038: val_acc did not improve from 0.99833\n",
      "Epoch 39/50\n",
      "2698/2698 [==============================] - 0s 177us/step - loss: 1.8281e-04 - acc: 0.9993 - val_loss: 7.5936e-04 - val_acc: 0.9980\n",
      "\n",
      "Epoch 00039: val_acc did not improve from 0.99833\n",
      "Epoch 40/50\n",
      "2698/2698 [==============================] - 0s 165us/step - loss: 2.5692e-06 - acc: 1.0000 - val_loss: 7.1439e-04 - val_acc: 0.9982\n",
      "\n",
      "Epoch 00040: val_acc did not improve from 0.99833\n",
      "Epoch 41/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2698/2698 [==============================] - 0s 168us/step - loss: 4.3190e-06 - acc: 1.0000 - val_loss: 7.4644e-04 - val_acc: 0.9981\n",
      "\n",
      "Epoch 00041: val_acc did not improve from 0.99833\n",
      "Epoch 42/50\n",
      "2698/2698 [==============================] - 0s 170us/step - loss: 5.3623e-07 - acc: 1.0000 - val_loss: 7.1562e-04 - val_acc: 0.9982\n",
      "\n",
      "Epoch 00042: val_acc did not improve from 0.99833\n",
      "Epoch 43/50\n",
      "2698/2698 [==============================] - 0s 152us/step - loss: 1.2954e-06 - acc: 1.0000 - val_loss: 7.7674e-04 - val_acc: 0.9979\n",
      "\n",
      "Epoch 00043: val_acc did not improve from 0.99833\n",
      "Epoch 44/50\n",
      "2698/2698 [==============================] - 1s 186us/step - loss: 5.1880e-06 - acc: 1.0000 - val_loss: 6.9603e-04 - val_acc: 0.9982\n",
      "\n",
      "Epoch 00044: val_acc did not improve from 0.99833\n",
      "Epoch 45/50\n",
      "2698/2698 [==============================] - 1s 192us/step - loss: 3.1398e-06 - acc: 1.0000 - val_loss: 7.0538e-04 - val_acc: 0.9982\n",
      "\n",
      "Epoch 00045: val_acc did not improve from 0.99833\n",
      "Epoch 46/50\n",
      "2698/2698 [==============================] - 1s 187us/step - loss: 3.8614e-10 - acc: 1.0000 - val_loss: 6.9252e-04 - val_acc: 0.9982\n",
      "\n",
      "Epoch 00046: val_acc did not improve from 0.99833\n",
      "Epoch 47/50\n",
      "2698/2698 [==============================] - 0s 177us/step - loss: 8.6361e-07 - acc: 1.0000 - val_loss: 7.1128e-04 - val_acc: 0.9982\n",
      "\n",
      "Epoch 00047: val_acc did not improve from 0.99833\n",
      "Epoch 48/50\n",
      "2698/2698 [==============================] - 0s 183us/step - loss: 9.5341e-11 - acc: 1.0000 - val_loss: 7.0202e-04 - val_acc: 0.9982\n",
      "\n",
      "Epoch 00048: val_acc did not improve from 0.99833\n",
      "Epoch 49/50\n",
      "2698/2698 [==============================] - 0s 154us/step - loss: 5.2391e-11 - acc: 1.0000 - val_loss: 6.8379e-04 - val_acc: 0.9982\n",
      "\n",
      "Epoch 00049: val_acc did not improve from 0.99833\n",
      "Epoch 50/50\n",
      "2698/2698 [==============================] - 0s 180us/step - loss: 2.3175e-11 - acc: 1.0000 - val_loss: 6.9178e-04 - val_acc: 0.9982\n",
      "\n",
      "Epoch 00050: val_acc did not improve from 0.99833\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from keras import Sequential\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.constraints import maxnorm\n",
    "from keras.layers import Dropout, Dense, np\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.utils import shuffle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import cohen_kappa_score as kappa_score\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from skopt import gp_minimize\n",
    "from skopt.space import Real, Integer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "kappa_scorer = make_scorer(kappa_score)\n",
    "from keras.constraints import maxnorm\n",
    "import os\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import glob\n",
    "import cv2\n",
    "import os\n",
    "from keras.models import Sequential\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.layers.core import Activation\n",
    "from keras.layers.core import Dropout\n",
    "from keras.layers.core import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Input\n",
    "from keras.models import Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers.core import Dense\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import concatenate\n",
    "import numpy as np\n",
    "import argparse\n",
    "import locale\n",
    "from keras.utils import np_utils\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "np.random.seed(1337)  # for reproducibility\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.optimizers import RMSprop\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from keras import Sequential\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.constraints import maxnorm\n",
    "from keras.layers import Dropout, Dense, np\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.utils import shuffle\n",
    "# CONSTANTS\n",
    "HIDDEN_UNITS = [100, 50, 25, 12]\n",
    "LABEL = 'AdoptionSpeed'\n",
    "TRAINING_TEST_SPLIT = 0.2\n",
    "RANDOM_NUMBER_SEED = 42\n",
    "N_CLASSES = 5\n",
    "EPOCHS = 100\n",
    "TRAIN_BATCH_SIZE = 10\n",
    "TRAIN_FILENAME = 'weights.best.hdf5'\n",
    "\n",
    "np.random.seed(RANDOM_NUMBER_SEED)\n",
    "\n",
    "\n",
    "def prepare_data(data):\n",
    "    pet_id = data.PetID\n",
    "\n",
    "    # Remove unused features\n",
    "    data.drop(['RescuerID', 'Description', 'PetID', 'State'], axis=1, inplace=True)\n",
    "\n",
    "    # Apply binning to ages\n",
    "    data['Age'] = pd.cut(data['Age'], [-1, 2, 3, 6, 255], labels=[0, 1, 2, 3])\n",
    "\n",
    "    # Apply binning to fee\n",
    "    data['Fee'] = pd.cut(data['Fee'], [-1, 50, 100, 200, 3000], labels=[0, 1, 2, 3])\n",
    "\n",
    "    # Apply binning to photo amount\n",
    "    data['PhotoAmt'] = pd.cut(data['PhotoAmt'], [-1, 1, 5, 10, 100], labels=[0, 1, 2, 3])\n",
    "\n",
    "    # Apply binning to video amount\n",
    "    data['VideoAmt'] = pd.cut(data['VideoAmt'], [-1, 1, 100], labels=[0, 1])\n",
    "\n",
    "    # Replace names with 1 is present, 0 if not present\n",
    "    data.loc[data['Name'].notnull(), 'Name'] = 1\n",
    "    data.loc[data['Name'].isnull(), 'Name'] = 0\n",
    "\n",
    "    # Fill missing continuous data\n",
    "    data_continuous = data.select_dtypes(exclude=['object'])\n",
    "    data_continuous.fillna(0, inplace=True)\n",
    "\n",
    "    # Fill missing string data\n",
    "    data_categorical = data.select_dtypes(include=['object'])\n",
    "    data_categorical.fillna('NONE', inplace=True)\n",
    "\n",
    "    final_data = data_continuous.merge(data_categorical, left_index=True, right_index=True)\n",
    "\n",
    "    return final_data, data_categorical, data_continuous, pet_id, data.shape[1]\n",
    "\n",
    "\n",
    "def create_mlp(input_dim, output_dim, dropout=0.3, arch=None):\n",
    "    # Default mlp architecture\n",
    "    \n",
    "    rmsprop = RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0)\n",
    "    model = Sequential([\n",
    "        Dense(64, input_dim=20),\n",
    "        Activation('relu'),\n",
    "        Dense(32),\n",
    "        Activation('relu'),\n",
    "        Dense(32),\n",
    "        Activation('relu'),\n",
    "        Dense(16),\n",
    "        Activation('relu'),\n",
    "        Dense(5),\n",
    "        Activation('sigmoid'),\n",
    "    ])\n",
    "    model.compile(optimizer=rmsprop,\n",
    "                  loss='mse',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Import and split\n",
    "    train, train_categorical, train_continuous, train_pet_id, training_dimension = prepare_data(\n",
    "        pd.read_csv('../all/train.csv'))\n",
    "    test, test_categorical, test_continuous, test_pet_id, _ = prepare_data(pd.read_csv('../all/test/test.csv'))\n",
    "\n",
    "    # Remove the outliers\n",
    "    clf = IsolationForest(max_samples=100, random_state=RANDOM_NUMBER_SEED)\n",
    "    clf.fit(train_continuous)\n",
    "    y_no_outliers = clf.predict(train_continuous)\n",
    "    y_no_outliers = pd.DataFrame(y_no_outliers, columns=['Top'])\n",
    "\n",
    "    train_continuous = train_continuous.iloc[y_no_outliers[y_no_outliers['Top'] == 1].index.values]\n",
    "    train_continuous.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    train_categorical = train_categorical.iloc[y_no_outliers[y_no_outliers['Top'] == 1].index.values]\n",
    "    train_categorical.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    train = train.iloc[y_no_outliers[y_no_outliers['Top'] == 1].index.values]\n",
    "    train.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Extract columns\n",
    "    columns = list(train_continuous.columns)\n",
    "\n",
    "    features_continuous = list(train_continuous.columns)\n",
    "    features_continuous.remove(LABEL)\n",
    "\n",
    "    features_categorical = list(train_categorical.columns)\n",
    "\n",
    "    # Extract matrices\n",
    "    matrix_train = np.matrix(train_continuous)\n",
    "    matrix_test = np.matrix(test_continuous)\n",
    "    matrix_test_no_label = np.matrix(train_continuous.drop(LABEL, axis=1))\n",
    "    matrix_y = np.array(train.AdoptionSpeed)\n",
    "\n",
    "    # Scale data\n",
    "    y_scaler = MinMaxScaler()\n",
    "    y_scaler.fit(matrix_y.reshape(matrix_y.shape[0], 1))\n",
    "\n",
    "    train_scaler = MinMaxScaler()\n",
    "    train_scaler.fit(matrix_train)\n",
    "\n",
    "    test_scaler = MinMaxScaler()\n",
    "    test_scaler.fit(matrix_test_no_label)\n",
    "\n",
    "    matrix_train_scaled = pd.DataFrame(train_scaler.transform(matrix_train), columns=columns)\n",
    "    test_matrix_scaled = pd.DataFrame(test_scaler.transform(matrix_test), columns=features_continuous)\n",
    "\n",
    "    train[columns] = pd.DataFrame(train_scaler.transform(matrix_train), columns=columns)\n",
    "    test[features_continuous] = test_matrix_scaled\n",
    "\n",
    "    # Extract continuous and categorical features\n",
    "    engineered_features = []\n",
    "\n",
    "    for continuous_feature in features_continuous:\n",
    "        engineered_features.append(tf.contrib.layers.real_valued_column(continuous_feature))\n",
    "\n",
    "    for categorical_feature in features_categorical:\n",
    "        sparse_column = tf.contrib.layers.sparse_column_with_hash_bucket(categorical_feature, hash_bucket_size=1000)\n",
    "\n",
    "        engineered_features.append(tf.contrib.layers.embedding_column(sparse_id_column=sparse_column,\n",
    "                                                                      dimension=16,\n",
    "                                                                      combiner='sum'))\n",
    "\n",
    "    # Split training set data between train and test\n",
    "    x_train, x_test, y_train, y_test = train_test_split(train[features_continuous + features_categorical],\n",
    "                                                        train[LABEL],\n",
    "                                                        test_size=0.8,\n",
    "                                                        random_state=RANDOM_NUMBER_SEED)\n",
    "\n",
    "    # Convert back to DataFrame\n",
    "    y_train = pd.DataFrame(y_train, columns=[LABEL])\n",
    "    x_train = pd.DataFrame(x_train, columns=features_continuous + features_categorical) \\\n",
    "        .merge(y_train, left_index=True, right_index=True)\n",
    "\n",
    "    y_test = pd.DataFrame(y_test, columns=[LABEL])\n",
    "    x_test = pd.DataFrame(x_test, columns=features_continuous + features_categorical) \\\n",
    "        .merge(y_test, left_index=True, right_index=True)\n",
    "\n",
    "    # Labels must be one-hot encoded for loss='categorical_crossentropy'\n",
    "    y_train_onehot = to_categorical(y_train, N_CLASSES)\n",
    "    y_test_onehot = to_categorical(y_test, N_CLASSES)\n",
    "\n",
    "    # Get neural network architecture and save to disk\n",
    "    model = create_mlp(input_dim=training_dimension, output_dim=N_CLASSES, arch=HIDDEN_UNITS)\n",
    "\n",
    "    with open(TRAIN_FILENAME, 'w') as f:\n",
    "        f.write(model.to_yaml())\n",
    "\n",
    "    # Output logs to tensorflow TensorBoard\n",
    "    # tensorboard = TensorBoard()\n",
    "\n",
    "    # only save model weights for best performing model\n",
    "    checkpoint = ModelCheckpoint(TRAIN_FILENAME,\n",
    "                                 monitor='val_acc',\n",
    "                                 verbose=1,\n",
    "                                 save_best_only=True)\n",
    "\n",
    "    # Stop training early if validation accuracy doesn't improve for long enough\n",
    "    early_stopping = EarlyStopping(monitor='val_acc', patience=500)\n",
    "\n",
    "    # Shuffle data for good measure before fitting\n",
    "    x_train, y_train_onehot = shuffle(x_train, y_train_onehot)\n",
    "\n",
    "    model.fit(x_train, y_train_onehot,validation_data = (x_test,y_test_onehot),epochs=50, batch_size=15,shuffle=True,\n",
    "              callbacks=[checkpoint, early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
